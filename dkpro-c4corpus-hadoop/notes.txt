Notes on improvements, experiments, things to be done, etc

- more efficient Hamming distance calculation & move inline. It's used O(n^2)
- 64-bit instead of 32-bit hash. old - Java hashcode(), first replacement FNVJ,
  but not even enough. Second try FNV.
- limit charset detector to first 8k bytes (correctness & efficiency)
- only read WARC header before doing record type & length checks
  
  
  
- All Hamming distances are even and <= 32. Why? Still need better hash? ** INVESTIGATE **
- Find more efficient WARC reader? Lemur Project, Java Web Archive Toolkit (JWAT), or iipc/webarchive-commons
- Need segment ID in mapper output name - done

First larger scale experiment
-----------------------------
GRRR - run with wrong crawl!! CC-MAIN-2015-27, not 2016-07
400 files (~1%), 1 m3.xlarge master, 2 m3.xlarge core @$0.10, 4 x c3.8xlarge @ $0.40
run time 2 hrs 19 min., 400 maps, 70 reducers, 
Cost - us-east-1e - $8.95 total, $2.98/hr, m3.xlarge = $0.0381 spot max, c3.8xlarge = $0.3338

Command line - de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob -D mapreduce.task.timeout=7200000 -D mapreduce.map.failures.maxpercent=5 -D mapreduce.map.maxattempts=2 -D c4corpus.keepminimalhtml=true s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2015-27/segments/*/warc/*-00[0-3]01-*.warc.gz s3://tfmorris/c4corpus/cc-phase1out-2016-07-1pct


Start    05:33
Map 100% 07:23:40
Red.100% 07:25:54
 (gap - S3 upload?)
End      07:43:16



** reducer 0 generated 549MB output (vs <10MB for others) and ran 20 min vs 1 min for others
[Caused by LongWritable.hashcode() being (int)value for simhash, truncating high order 32 bits]

**Maps 376-379 output only 3MB. Inputs all ~1GB.
https://s3.amazonaws.com/tfmorris/c4corpus/cc-phase1out-2016-07-1pct/CC-MAIN-20150627031814-00301-ip-10-179-60-89.ec2.internal-m-00376.seg-00000.warc.gz
https://s3.amazonaws.com/tfmorris/c4corpus/cc-phase1out-2016-07-1pct/CC-MAIN-20150627031814-00101-ip-10-179-60-89.ec2.internal-m-00378.seg-00000.warc.gz
https://s3.amazonaws.com/tfmorris/c4corpus/cc-phase1out-2016-07-1pct/CC-MAIN-20150627031814-00201-ip-10-179-60-89.ec2.internal-m-00379.seg-00000.warc.gz
372 8MB - https://s3.amazonaws.com/tfmorris/c4corpus/cc-phase1out-2016-07-1pct/CC-MAIN-20150627031817-00201-ip-10-179-60-89.ec2.internal-m-00372.seg-00000.warc.gz
373 8MB - https://s3.amazonaws.com/tfmorris/c4corpus/cc-phase1out-2016-07-1pct/CC-MAIN-20150627031817-00101-ip-10-179-60-89.ec2.internal-m-00373.seg-00000.warc.gz
374 8MB - https://s3.amazonaws.com/tfmorris/c4corpus/cc-phase1out-2016-07-1pct/CC-MAIN-20150627031817-00001-ip-10-179-60-89.ec2.internal-m-00374.seg-00000.warc.gz
375 7MB - https://s3.amazonaws.com/tfmorris/c4corpus/cc-phase1out-2016-07-1pct/CC-MAIN-20150627031817-00301-ip-10-179-60-89.ec2.internal-m-00375.seg-00000.warc.gz
368 28MB - https://s3.amazonaws.com/tfmorris/c4corpus/cc-phase1out-2016-07-1pct/CC-MAIN-20150627031817-00101-ip-10-179-60-89.ec2.internal-m-00368.seg-00000.warc.gz
369 27MB - https://s3.amazonaws.com/tfmorris/c4corpus/cc-phase1out-2016-07-1pct/CC-MAIN-20150627031817-00001-ip-10-179-60-89.ec2.internal-m-00369.seg-00000.warc.gz
 

Map 356 failed twice (permanent) 
  file: s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2015-27/segments/1435375093899.18/warc/CC-MAIN-20150627031813-00201-ip-10-179-60-89.ec2.internal.warc.gz
2016-04-03 06:50:59,141 INFO [main] de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$MapperClass: Processed 1000 records, total length 4167848 characters
2016-04-03 06:51:26,214 INFO [main] de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$MapperClass: Processed 2000 records, total length 8596008 characters
...
2016-04-03 06:53:18,267 INFO [main] de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$MapperClass: Processed 6000 records, total length 24613681 characters
2016-04-03 06:53:44,562 INFO [main] de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$MapperClass: Processed 7000 records, total length 28770330 characters
[abort after 7k records]

** INVESTIGATE MAP FAILURE with local copy of the file **
Theory - WARCRecord reads entire record into memory when it should only read header before size check
[Actually a deeply nested HTML doc (SQL stack trace from server) which triggered O(n!) in nesting levels behavior of boilerplate]

                FILE: Number of bytes written=6427027402
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=90134
                HDFS: Number of bytes written=0
                HDFS: Number of read operations=399
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=0
                S3: Number of bytes read=  372380725845
                S3: Number of bytes written=20989896179
                S3: Number of read operations=0
                S3: Number of large read operations=0
                S3: Number of write operations=0
        Job Counters 
               *Failed map tasks=2*
                Killed map tasks=8
                Killed reduce tasks=1
                Launched map tasks=409
                Launched reduce tasks=72
                Other local map tasks=1
                Data-local map tasks=408
                Total time spent by all maps in occupied slots (ms)=28288246950
                Total time spent by all reduces in occupied slots (ms)=18706527810
                Total time spent by all map tasks (ms)=   628627710 (174 hrs, avg 25m30s per map)
                Total time spent by all reduce tasks (ms)=207850309
                Total vcore-seconds taken by all map tasks=628627710
                Total vcore-seconds taken by all reduce tasks=207850309
                Total megabyte-seconds taken by all map tasks=905223902400
                Total megabyte-seconds taken by all reduce tasks=598608889920
        Map-Reduce Framework
                Map input records= 64832529
                Map output records=54752216
                Map output bytes=4588970268
                Map output materialized bytes=3394190271
                Reduce input groups=255823
                Reduce shuffle bytes=3394190271
                Reduce input records=54752216
                Reduce output records=30808334
                Spilled Records=109504432
                Shuffled Maps =28329
                Merged Map outputs=28329
                GC time elapsed (ms)=13061006
                CPU time spent (ms)=578848380
                Physical memory (bytes) snapshot=524065198080
                Virtual memory (bytes) snapshot=1643861057536
                Total committed heap usage (bytes)=523228938240
        de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$C4_COUNTER
                Map input records=                    64832529 (64.8M)
                M1_WARC_NOT_HTTP_RESPONSE=            43221819
                M3_WARC_WRONG_CONTENT_TYPE=             666808
                M4_WARC_EMPTY_TEXT=                    7255848
                M5_WARC_OUTPUT_RECORDS=               13688054 (13.7M 21% of input)
                Map output records (4x slices) =      54752216
 (all reducer counts, except total comparisons, are double counted by up to 4x)
                R_SIMHASH_CANDIDATE_NOT_DUPLICATE= 19259842231
                R_SIMHASH_COMPARISONS=             19271582261 (19.3B)
                R_SIMHASH_EXACT_DUPLICATE=            19560389 
                R_SIMHASH_HASH_DIFFERENT_LANGUAGE=      365369
                R_SIMHASH_HASH_DIFFERENT_LENGTH=       9213563
                R_SIMHASH_NEAR_DUPLICATE=             11247945
                R_SIMHASH_NEAR_DUPLICATE_DIFF_LANG=     492085
        Phase1FullJob$C4_HAMMING_DIST (Histogram of Hamming distance)
                D04=  67939014 (why all even?)
                D06= 263119484
                D08= 741747843
                D10=1593646977
                D12=2668311135
                D14=3529389267
                D16=3701571963
                D18=3082943862
                D20=2024420469
                D22=1037517297
                D24= 405567292
                D26= 117105572
                D28=  23464916
                D30=   2926273
                D32=    170214 (why nothing greater than 32?)
        File Input Format Counters 
                Bytes Read=372380725845 (372 GB)
        File Output Format Counters 
                Bytes Written=1136452944 (1.1 GB)

30808334 dupe records written by reducers
15614719 (50.6%) in part-r-00000
25809780 unique dupe records (?this # is too high - ie more than total # of recods?)
21253056 occur 1 time
 4175567 occur 2 times
  320484 3 times
   60673 4 times

Run #2 with mapper HTML deep nesting bug fixed
================================================
Crawl: 2016-07 1.1% sample (400 files)
Creation date:2016-04-09 21:07 (UTC)
End date:     2016-04-10 02:57 (UTC)
Elapsed time: 5 hrs, 49 minutes (20335 seconds) !!
Normalized Instance Hours: 1680 
Cost: $19.50 c3.xlarge spot price $0.30-0.35 (bid $0.45)


============= 2nd run of 1% sample =========

21:07 Start 471 total tasks, 400 mappers, 70 reducers
21:53-57 1st wave of mappers completes 470->330
22:15-25 2nd wave of mappers completes 330-
22:47 reducers running (when did they start)?
23:20 containers pending down to 8, but then rebound to 32
24:30 containers pending down to 5
25:40 pending = 0 remain there for another hour until termination
26:55 termination after 5h49m - 20335 seconds


cancelled maps 39, 43, 83, 93, 106, 111, 121, 182, 218, 261, 281
pending maps 102 (two failed attempts - after 1000+ records), 53 (two failed attempts - after 1000+ records)
73 running tasks

mapper 000 - succeeded after 39 minutes, 34,357 records

AttemptID:attempt_1460236413688_0001_m_000001_0 Timed out after 7200 secs
Same for:
000001 - failed, killed, succeeded after 91.92 minutes 34,436 records - S3 reset - s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701159376.39/warc/CC-MAIN-20160205193919-00301-ip-10-236-182-209.ec2.internal.warc.gz
000038 - failed, failed, killed - 12,000 records - s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701167113.3/warc/CC-MAIN-20160205193927-00101-ip-10-236-182-209.ec2.internal.warc.gz
000043 - failed, failed, killed - 5,000 records, S3 reset - s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701163421.31/warc/CC-MAIN-20160205193923-00001-ip-10-236-182-209.ec2.internal.warc.gz
000053 - failed, failed, pending - 1000 records - s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701161942.67/warc/CC-MAIN-20160205193921-00001-ip-10-236-182-209.ec2.internal.warc.gz
000083 - failed, failed, killed - 7000 records - s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701158601.61/warc/CC-MAIN-20160205193918-00301-ip-10-236-182-209.ec2.internal.warc.gz
000093 - failed, failed, killed - 1000 records, then 1hr stall - s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701160918.28/warc/CC-MAIN-20160205193920-00201-ip-10-236-182-209.ec2.internal.warc.gz
000102 - failed, failed, pending - 1000 records, then 1 hr stall - s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701166261.11/warc/CC-MAIN-20160205193926-00101-ip-10-236-182-209.ec2.internal.warc.gz
000106 - killed
000111 - killed
000121 - killed
000182 - failed, failed, killed - 3000 records - s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701163663.52/warc/CC-MAIN-20160205193923-00001-ip-10-236-182-209.ec2.internal.warc.gz
000218 - killed
000261 - killed
000281 - killed
000307 - failed, succeeded, killed - S3 connect reset during first 1000 records, then finished 32,375 records in 86.5 minutes
000338 - failed, failed, killed - 1000 records - s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701157012.30/warc/CC-MAIN-20160205193917-00101-ip-10-236-182-209.ec2.internal.warc.gz


? Speculating that there are some indications of S3 problems getting to the log bucket, so switching from s3n: protocol to s3: ?

** Need to include segment ID in output file path so there are no collisions ** - fixed 14-April

2016-04-10 02:54:03,061 INFO org.apache.hadoop.mapreduce.Job (main): Job job_1460236413688_0001 completed successfully
2016-04-10 02:54:03,173 INFO org.apache.hadoop.mapreduce.Job (main): Counters: 83
        File System Counters
                FILE: Number of bytes read=   2756948739 (2.7GB)
                FILE: Number of bytes written=6001403664
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=87969
                HDFS: Number of bytes written=0
                HDFS: Number of read operations=386
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=0
                S3: Number of bytes read=  347534627080 (347.5 GB)
                S3: Number of bytes written=21559744696 (21.6 GB)
                S3: Number of read operations=0
                S3: Number of large read operations=0
                S3: Number of write operations=0
        Job Counters 
                **Failed map tasks=30 (?why is this higher than specified 5% max failures?)
                **Killed map tasks=15
                Launched map tasks=431
                Launched reduce tasks=71
                Other local map tasks=29
                Data-local map tasks=402
                Total time spent by all maps in occupied slots (ms)=   47824020180
                Total time spent by all reduces in occupied slots (ms)=91888404840
                Total time spent by all map tasks (ms)=   1062756004 (295 hrs! vs 174 hrs in run #1 - 41.2 min/map vs 25.5 min)
                Total time spent by all reduce tasks (ms)=1020982276
                Total vcore-seconds taken by all map tasks=1062756004
                Total vcore-seconds taken by all reduce tasks=1020982276
                Total megabyte-seconds taken by all map tasks=1530368645760
                Total megabyte-seconds taken by all reduce tasks=2940428954880
        Map-Reduce Framework
                Map input records= 57729785
                Map output records=50316560
                Map output bytes=4221397508
                Map output materialized bytes=3187158858
                Input split bytes=87969
                Reduce input groups=251557
                Reduce shuffle bytes=3187158858
                Reduce input records= 50316560
                Reduce output records=26766031
                Spilled Records=100633120
                Shuffled Maps =27406
                Merged Map outputs=27406
                GC time elapsed (ms)=12730924
                CPU time spent (ms)=777520170
                Physical memory (bytes) snapshot=517031022592
                Virtual memory (bytes) snapshot=1601591750656
                Total committed heap usage (bytes)=496871407616

        de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$C4_COUNTER
                Map input records=                   57729785 (57.7M)
                M2_WARC_NOT_HTTP_RESPONSE=           38486652
                M3_WARC_WRONG_CONTENT_TYPE=            486346
                M4_WARC_EMPTY_TEXT=                   6177646
                M5_WARC_OUTPUT_RECORDS=              12579140 (12.6M)
                Reduce input records=                50316560 (4x mapped records)
                R_SIMHASH_CANDIDATE_NOT_DUPLICATE=18086438372
                R_SIMHASH_COMPARISONS=            18097957709 (18 billion)
                R_SIMHASH_EXACT_DUPLICATE=           15843480
                R_SIMHASH_HASH_DIFFERENT_LANGUAGE=     248849
                R_SIMHASH_HASH_DIFFERENT_LENGTH=      8694981
                R_SIMHASH_NEAR_DUPLICATE=            10922551
                R_SIMHASH_NEAR_DUPLICATE_DIFF_LANG=    596786
        de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$C4_HAMMING_DIST
                D04=  65803334
                D06= 253499154
                D08= 710306749
                D10=1519249391
                D12=2527691783
                D14=3327260297
                D16=3468617219
                D18=2875080043
                D20=1877563038
                D22= 957670024
                D24= 372497327
                D26= 107033271
                D28=  21362546
                D30=   2649366
                D32=    153994
        File Input Format Counters 
                Bytes Read=347534627080 (347.6 GB)
        File Output Format Counters 
                Bytes Written=1037290680 (1 GB)

======== 3rd run =========
2016-04-13 

15:55 (UTC) Start
16:03 780 pending containers per CloudWatch (false reading from startup?)
16:07 484 pending, 159 allocated containers, 471 total tasks (400 map, 70 reduce, 1 master?)

Aborted before 2 hrs due to lack of progress.
Profiling revealed two significant issues:
1. 50% time being spent in constructor for java.net.URL() as part of HREF absolutization - removed all href attributes
2. 35% time being spent in LinkedList.get() by contextual reclassifier - switched from LinkedList to ArrayList

======= 4th run ========
2016-04-13
switched to 16x c3.2xlarge instead of 4x8xlarge due to spot price bid jump
Elapsed time 1h43m, 560 normalized instance hours, map reduce job 1hr31min - down from 2h19 min originally, but with zero failures now

20:59 (UTC) start
21:11 container pending drops from 780 to 510 (according to CloudWatch)
21:10 95% CPU utilization according to Ganglia
21:25 first container finishes 
21:30 CPU utilization drops to 70%
21:31 second round of containers started, pending down to 350
21:43 first container of second round finishes
21:45 CPU utilization drops again to 50%
21:50 pending containers down to 200
21:54 222 tasks completed, 107 running, 0 failed, 142 pending (from EMR dashboard)
map tasks taking 18-20 minutes mostly, with some outliers at 30 minutes
22:01 253 tasks complete, 107 running, 0 failed, 111 pending
22:03 container pending <100
22:12 50 containers pending
22:10 285 completed, 107 running, 79 pending, 33 reducers running, 37 pending
22:10 ~40 containers pending, graph flat 'til 22:20
22:13 302 complete, 107 running, 62 pending, still 33 reducers running
22:20 343 complete, 96 running, 32 pending, 33 reducers
22:27 357 complete, 89 running, 25 pending, 46 reducers running
22:32 293 complete, 75 running, 4 pending (all maps), all 70 reducers running
22:35 zero pending tasks
22:40 all tasks complete (not sure how long ago)


2016-04-13 22:37:49,023 INFO org.apache.hadoop.mapreduce.Job (main): Job job_1460581468620_0001 completed successfully
2016-04-13 22:37:49,125 INFO org.apache.hadoop.mapreduce.Job (main): Counters: 80
        File System Counters
                FILE: Number of bytes read=   3592936416
                FILE: Number of bytes written=7749905591
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=91160
                HDFS: Number of bytes written=0
                HDFS: Number of read operations=400
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=0
                S3: Number of bytes read=  360221622683 (360GB)
                S3: Number of bytes written=31037679194  (31GB)
                S3: Number of read operations=0
                S3: Number of large read operations=0
                S3: Number of write operations=0
        Job Counters 
                Launched map tasks=400
                Launched reduce tasks=71
                Data-local map tasks=400
                Total time spent by all maps in occupied slots (ms)=19656362385
                Total time spent by all reduces in occupied slots (ms)=13649966100
                Total time spent by all map tasks (ms)=    436808053 (121hrs, 18.2 min/map avg)
                Total time spent by all reduce tasks (ms)= 151666290 (42 hrs, 36.1 min/reduce avg)
                Total vcore-seconds taken by all map tasks=436808053
                Total vcore-seconds taken by all reduce tasks=151666290
                Total megabyte-seconds taken by all map tasks=629003596320
                Total megabyte-seconds taken by all reduce tasks=436798915200
        Map-Reduce Framework
                Map input records= 59828572
                Map output records=63929664
                Map output bytes=5372975580
                Map output materialized bytes=4097907396
                Input split bytes=91160
                Combine input records=0
                Combine output records=0
                Reduce input groups=251953
                Reduce shuffle bytes=4097907396
                Reduce input records=63929664
                Reduce output records=29890685
                Spilled Records=127859328
                Shuffled Maps =28400
                Failed Shuffles=0
                Merged Map outputs=28400
                GC time elapsed (ms)=3533158
                CPU time spent (ms)=422480670
                Physical memory (bytes) snapshot=441715634176
                Virtual memory (bytes) snapshot=1606043619328
                Total committed heap usage (bytes)=379276754944

        de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$C4_COUNTER
                Map input records=                   59828572 (59.8M)
                M2_WARC_NOT_HTTP_RESPONSE=           39885848
                M3_WARC_WRONG_CONTENT_TYPE=            504128
                M4_WARC_EMPTY_TEXT=                   3456179
                M5_WARC_OUTPUT_RECORDS=              15982416 (15.9M)
                Reduce input records=                63929664 (63.9M)
                R_SIMHASH_CANDIDATE_NOT_DUPLICATE=27146842860
                R_SIMHASH_COMPARISONS=            27165069812 (27Billion)
                R_SIMHASH_EXACT_DUPLICATE=           13255019
                R_SIMHASH_HASH_DIFFERENT_LANGUAGE=     480767
                R_SIMHASH_HASH_DIFFERENT_LENGTH=     15745579
                R_SIMHASH_NEAR_DUPLICATE=            16635666
                R_SIMHASH_NEAR_DUPLICATE_DIFF_LANG=   1591286
        de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$C4_HAMMING_DIST
                D04= 103009309
                D06= 393052862
                D08=1094666125
                D10=2318226500
                D12=3833764476
                D14=5002933159
                D16=5191323091
                D18=4274616529
                D20=2782726049
                D22=1412166159
                D24= 548093229
                D26= 156899445
                D28=  31270013
                D30=   3871677
                D32=    223463
        File Input Format Counters 
                Bytes Read=360221622683
        File Output Format Counters 
                Bytes Written=1242889294
 
=== run 4 with both current & original code ===
aborted by outbid

Gradual reduction in CPU utilization is probably caused by reduce jobs getting started "too early" is due to this default value:

    mapreduce.job.reduce.slowstart.completedmaps	0.05

Since our Phase 1 is almost a map-only job, we want this to be much closer to 100% (ie 1.0)
*confirmed* - changing this to 0.9 on job submission makes much better use of the CPUs.