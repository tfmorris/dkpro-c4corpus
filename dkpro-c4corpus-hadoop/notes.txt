Notes on improvements, experiments, things to be done, etc

- more efficient Hamming distance calculation & move inline. It's used O(n^2)
- 64-bit instead of 32-bit hash. old - Java hashcode(), first replacement FNVJ,
  but not even enough. Second try FNV.
- limit charset detector to first 8k bytes (correctness & efficiency)
- only read WARC header before doing record type & length checks
  
  
  
- All Hamming distances are even and <= 32. Why? Still need better hash? ** INVESTIGATE **
- Find more efficient WARC reader? Lemur Project, Java Web Archive Toolkit (JWAT), or iipc/webarchive-commons
- Need segment ID in mapper output name - done

First larger scale experiment
-----------------------------
GRRR - run with wrong crawl!! CC-MAIN-2015-27, not 2016-07
400 files (~1%), 1 m3.xlarge master, 2 m3.xlarge core @$0.10, 4 x c3.8xlarge @ $0.40
run time 2 hrs 19 min., 400 maps, 70 reducers, 
Cost - us-east-1e - $8.95 total, $2.98/hr, m3.xlarge = $0.0381 spot max, c3.8xlarge = $0.3338

Command line - de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob -D mapreduce.task.timeout=7200000 -D mapreduce.map.failures.maxpercent=5 -D mapreduce.map.maxattempts=2 -D c4corpus.keepminimalhtml=true s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2015-27/segments/*/warc/*-00[0-3]01-*.warc.gz s3://tfmorris/c4corpus/cc-phase1out-2016-07-1pct


Start    05:33
Map 100% 07:23:40
Red.100% 07:25:54
 (gap - S3 upload?)
End      07:43:16



** reducer 0 generated 549MB output (vs <10MB for others) and ran 20 min vs 1 min for others
[Caused by LongWritable.hashcode() being (int)value for simhash, truncating high order 32 bits]

**Maps 376-379 output only 3MB. Inputs all ~1GB.
https://s3.amazonaws.com/tfmorris/c4corpus/cc-phase1out-2016-07-1pct/CC-MAIN-20150627031814-00301-ip-10-179-60-89.ec2.internal-m-00376.seg-00000.warc.gz
https://s3.amazonaws.com/tfmorris/c4corpus/cc-phase1out-2016-07-1pct/CC-MAIN-20150627031814-00101-ip-10-179-60-89.ec2.internal-m-00378.seg-00000.warc.gz
https://s3.amazonaws.com/tfmorris/c4corpus/cc-phase1out-2016-07-1pct/CC-MAIN-20150627031814-00201-ip-10-179-60-89.ec2.internal-m-00379.seg-00000.warc.gz
372 8MB - https://s3.amazonaws.com/tfmorris/c4corpus/cc-phase1out-2016-07-1pct/CC-MAIN-20150627031817-00201-ip-10-179-60-89.ec2.internal-m-00372.seg-00000.warc.gz
373 8MB - https://s3.amazonaws.com/tfmorris/c4corpus/cc-phase1out-2016-07-1pct/CC-MAIN-20150627031817-00101-ip-10-179-60-89.ec2.internal-m-00373.seg-00000.warc.gz
374 8MB - https://s3.amazonaws.com/tfmorris/c4corpus/cc-phase1out-2016-07-1pct/CC-MAIN-20150627031817-00001-ip-10-179-60-89.ec2.internal-m-00374.seg-00000.warc.gz
375 7MB - https://s3.amazonaws.com/tfmorris/c4corpus/cc-phase1out-2016-07-1pct/CC-MAIN-20150627031817-00301-ip-10-179-60-89.ec2.internal-m-00375.seg-00000.warc.gz
368 28MB - https://s3.amazonaws.com/tfmorris/c4corpus/cc-phase1out-2016-07-1pct/CC-MAIN-20150627031817-00101-ip-10-179-60-89.ec2.internal-m-00368.seg-00000.warc.gz
369 27MB - https://s3.amazonaws.com/tfmorris/c4corpus/cc-phase1out-2016-07-1pct/CC-MAIN-20150627031817-00001-ip-10-179-60-89.ec2.internal-m-00369.seg-00000.warc.gz
 

Map 356 failed twice (permanent) 
  file: s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2015-27/segments/1435375093899.18/warc/CC-MAIN-20150627031813-00201-ip-10-179-60-89.ec2.internal.warc.gz
2016-04-03 06:50:59,141 INFO [main] de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$MapperClass: Processed 1000 records, total length 4167848 characters
2016-04-03 06:51:26,214 INFO [main] de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$MapperClass: Processed 2000 records, total length 8596008 characters
...
2016-04-03 06:53:18,267 INFO [main] de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$MapperClass: Processed 6000 records, total length 24613681 characters
2016-04-03 06:53:44,562 INFO [main] de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$MapperClass: Processed 7000 records, total length 28770330 characters
[abort after 7k records]

** INVESTIGATE MAP FAILURE with local copy of the file **
Theory - WARCRecord reads entire record into memory when it should only read header before size check
[Actually a deeply nested HTML doc (SQL stack trace from server) which triggered O(n!) in nesting levels behavior of boilerplate]

                FILE: Number of bytes written=6427027402
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=90134
                HDFS: Number of bytes written=0
                HDFS: Number of read operations=399
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=0
                S3: Number of bytes read=  372380725845
                S3: Number of bytes written=20989896179
                S3: Number of read operations=0
                S3: Number of large read operations=0
                S3: Number of write operations=0
        Job Counters 
               *Failed map tasks=2*
                Killed map tasks=8
                Killed reduce tasks=1
                Launched map tasks=409
                Launched reduce tasks=72
                Other local map tasks=1
                Data-local map tasks=408
                Total time spent by all maps in occupied slots (ms)=28288246950
                Total time spent by all reduces in occupied slots (ms)=18706527810
                Total time spent by all map tasks (ms)=   628627710 (174 hrs, avg 25m30s per map)
                Total time spent by all reduce tasks (ms)=207850309
                Total vcore-seconds taken by all map tasks=628627710
                Total vcore-seconds taken by all reduce tasks=207850309
                Total megabyte-seconds taken by all map tasks=905223902400
                Total megabyte-seconds taken by all reduce tasks=598608889920
        Map-Reduce Framework
                Map input records= 64832529
                Map output records=54752216
                Map output bytes=4588970268
                Map output materialized bytes=3394190271
                Reduce input groups=255823
                Reduce shuffle bytes=3394190271
                Reduce input records=54752216
                Reduce output records=30808334
                Spilled Records=109504432
                Shuffled Maps =28329
                Merged Map outputs=28329
                GC time elapsed (ms)=13061006
                CPU time spent (ms)=578848380
                Physical memory (bytes) snapshot=524065198080
                Virtual memory (bytes) snapshot=1643861057536
                Total committed heap usage (bytes)=523228938240
        de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$C4_COUNTER
                Map input records=                    64832529 (64.8M)
                M1_WARC_NOT_HTTP_RESPONSE=            43221819
                M3_WARC_WRONG_CONTENT_TYPE=             666808
                M4_WARC_EMPTY_TEXT=                    7255848
                M5_WARC_OUTPUT_RECORDS=               13688054 (13.7M 21% of input)
                Map output records (4x slices) =      54752216
 (all reducer counts, except total comparisons, are double counted by up to 4x)
                R_SIMHASH_CANDIDATE_NOT_DUPLICATE= 19259842231
                R_SIMHASH_COMPARISONS=             19271582261 (19.3B)
                R_SIMHASH_EXACT_DUPLICATE=            19560389 
                R_SIMHASH_HASH_DIFFERENT_LANGUAGE=      365369
                R_SIMHASH_HASH_DIFFERENT_LENGTH=       9213563
                R_SIMHASH_NEAR_DUPLICATE=             11247945
                R_SIMHASH_NEAR_DUPLICATE_DIFF_LANG=     492085
        Phase1FullJob$C4_HAMMING_DIST (Histogram of Hamming distance)
                D04=  67939014 (why all even?)
                D06= 263119484
                D08= 741747843
                D10=1593646977
                D12=2668311135
                D14=3529389267
                D16=3701571963
                D18=3082943862
                D20=2024420469
                D22=1037517297
                D24= 405567292
                D26= 117105572
                D28=  23464916
                D30=   2926273
                D32=    170214 (why nothing greater than 32?)
        File Input Format Counters 
                Bytes Read=372380725845 (372 GB)
        File Output Format Counters 
                Bytes Written=1136452944 (1.1 GB)

30808334 dupe records written by reducers
15614719 (50.6%) in part-r-00000
25809780 unique dupe records (?this # is too high - ie more than total # of recods?)
21253056 occur 1 time
 4175567 occur 2 times
  320484 3 times
   60673 4 times

Run #2 with mapper HTML deep nesting bug fixed
================================================
Crawl: 2016-07 1.1% sample (400 files)
Creation date:2016-04-09 21:07 (UTC)
End date:     2016-04-10 02:57 (UTC)
Elapsed time: 5 hrs, 49 minutes (20335 seconds) !!
Normalized Instance Hours: 1680 
Cost: $19.50 c3.xlarge spot price $0.30-0.35 (bid $0.45)


============= 2nd run of 1% sample =========

21:07 Start 471 total tasks, 400 mappers, 70 reducers
21:53-57 1st wave of mappers completes 470->330
22:15-25 2nd wave of mappers completes 330-
22:47 reducers running (when did they start)?
23:20 containers pending down to 8, but then rebound to 32
24:30 containers pending down to 5
25:40 pending = 0 remain there for another hour until termination
26:55 termination after 5h49m - 20335 seconds


cancelled maps 39, 43, 83, 93, 106, 111, 121, 182, 218, 261, 281
pending maps 102 (two failed attempts - after 1000+ records), 53 (two failed attempts - after 1000+ records)
73 running tasks

mapper 000 - succeeded after 39 minutes, 34,357 records

AttemptID:attempt_1460236413688_0001_m_000001_0 Timed out after 7200 secs
Same for:
000001 - failed, killed, succeeded after 91.92 minutes 34,436 records - S3 reset - s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701159376.39/warc/CC-MAIN-20160205193919-00301-ip-10-236-182-209.ec2.internal.warc.gz
000038 - failed, failed, killed - 12,000 records - s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701167113.3/warc/CC-MAIN-20160205193927-00101-ip-10-236-182-209.ec2.internal.warc.gz
000043 - failed, failed, killed - 5,000 records, S3 reset - s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701163421.31/warc/CC-MAIN-20160205193923-00001-ip-10-236-182-209.ec2.internal.warc.gz
000053 - failed, failed, pending - 1000 records - s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701161942.67/warc/CC-MAIN-20160205193921-00001-ip-10-236-182-209.ec2.internal.warc.gz
000083 - failed, failed, killed - 7000 records - s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701158601.61/warc/CC-MAIN-20160205193918-00301-ip-10-236-182-209.ec2.internal.warc.gz
000093 - failed, failed, killed - 1000 records, then 1hr stall - s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701160918.28/warc/CC-MAIN-20160205193920-00201-ip-10-236-182-209.ec2.internal.warc.gz
000102 - failed, failed, pending - 1000 records, then 1 hr stall - s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701166261.11/warc/CC-MAIN-20160205193926-00101-ip-10-236-182-209.ec2.internal.warc.gz
000106 - killed
000111 - killed
000121 - killed
000182 - failed, failed, killed - 3000 records - s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701163663.52/warc/CC-MAIN-20160205193923-00001-ip-10-236-182-209.ec2.internal.warc.gz
000218 - killed
000261 - killed
000281 - killed
000307 - failed, succeeded, killed - S3 connect reset during first 1000 records, then finished 32,375 records in 86.5 minutes
000338 - failed, failed, killed - 1000 records - s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701157012.30/warc/CC-MAIN-20160205193917-00101-ip-10-236-182-209.ec2.internal.warc.gz


? Speculating that there are some indications of S3 problems getting to the log bucket, so switching from s3n: protocol to s3: ?

** Need to include segment ID in output file path so there are no collisions ** - fixed 14-April

2016-04-10 02:54:03,061 INFO org.apache.hadoop.mapreduce.Job (main): Job job_1460236413688_0001 completed successfully
2016-04-10 02:54:03,173 INFO org.apache.hadoop.mapreduce.Job (main): Counters: 83
        File System Counters
                FILE: Number of bytes read=   2756948739 (2.7GB)
                FILE: Number of bytes written=6001403664
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=87969
                HDFS: Number of bytes written=0
                HDFS: Number of read operations=386
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=0
                S3: Number of bytes read=  347534627080 (347.5 GB)
                S3: Number of bytes written=21559744696 (21.6 GB)
                S3: Number of read operations=0
                S3: Number of large read operations=0
                S3: Number of write operations=0
        Job Counters 
                **Failed map tasks=30 (?why is this higher than specified 5% max failures?)
                **Killed map tasks=15
                Launched map tasks=431
                Launched reduce tasks=71
                Other local map tasks=29
                Data-local map tasks=402
                Total time spent by all maps in occupied slots (ms)=   47824020180
                Total time spent by all reduces in occupied slots (ms)=91888404840
                Total time spent by all map tasks (ms)=   1062756004 (295 hrs! vs 174 hrs in run #1 - 41.2 min/map vs 25.5 min)
                Total time spent by all reduce tasks (ms)=1020982276
                Total vcore-seconds taken by all map tasks=1062756004
                Total vcore-seconds taken by all reduce tasks=1020982276
                Total megabyte-seconds taken by all map tasks=1530368645760
                Total megabyte-seconds taken by all reduce tasks=2940428954880
        Map-Reduce Framework
                Map input records= 57729785
                Map output records=50316560
                Map output bytes=4221397508
                Map output materialized bytes=3187158858
                Input split bytes=87969
                Reduce input groups=251557
                Reduce shuffle bytes=3187158858
                Reduce input records= 50316560
                Reduce output records=26766031
                Spilled Records=100633120
                Shuffled Maps =27406
                Merged Map outputs=27406
                GC time elapsed (ms)=12730924
                CPU time spent (ms)=777520170
                Physical memory (bytes) snapshot=517031022592
                Virtual memory (bytes) snapshot=1601591750656
                Total committed heap usage (bytes)=496871407616

        de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$C4_COUNTER
                Map input records=                   57729785 (57.7M)
                M2_WARC_NOT_HTTP_RESPONSE=           38486652
                M3_WARC_WRONG_CONTENT_TYPE=            486346
                M4_WARC_EMPTY_TEXT=                   6177646
                M5_WARC_OUTPUT_RECORDS=              12579140 (12.6M)
                Reduce input records=                50316560 (4x mapped records)
                R_SIMHASH_CANDIDATE_NOT_DUPLICATE=18086438372
                R_SIMHASH_COMPARISONS=            18097957709 (18 billion)
                R_SIMHASH_EXACT_DUPLICATE=           15843480
                R_SIMHASH_HASH_DIFFERENT_LANGUAGE=     248849
                R_SIMHASH_HASH_DIFFERENT_LENGTH=      8694981
                R_SIMHASH_NEAR_DUPLICATE=            10922551
                R_SIMHASH_NEAR_DUPLICATE_DIFF_LANG=    596786
        de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$C4_HAMMING_DIST
                D04=  65803334
                D06= 253499154
                D08= 710306749
                D10=1519249391
                D12=2527691783
                D14=3327260297
                D16=3468617219
                D18=2875080043
                D20=1877563038
                D22= 957670024
                D24= 372497327
                D26= 107033271
                D28=  21362546
                D30=   2649366
                D32=    153994
        File Input Format Counters 
                Bytes Read=347534627080 (347.6 GB)
        File Output Format Counters 
                Bytes Written=1037290680 (1 GB)

======== 3rd run =========
2016-04-13 

15:55 (UTC) Start
16:03 780 pending containers per CloudWatch (false reading from startup?)
16:07 484 pending, 159 allocated containers, 471 total tasks (400 map, 70 reduce, 1 master?)

Aborted before 2 hrs due to lack of progress.
Profiling revealed two significant issues:
1. 50% time being spent in constructor for java.net.URL() as part of HREF absolutization - removed all href attributes
2. 35% time being spent in LinkedList.get() by contextual reclassifier - switched from LinkedList to ArrayList

======= 4th run ========
2016-04-13
aborted by outbid

=== run 5 with both current & original code ===
2016-04-13

switched to 16x c3.2xlarge instead of 4x8xlarge due to spot price bid jump

Elapsed time 1h43m, 560 normalized instance hours, map reduce job 1hr31min - down from 2h19 min originally, but with zero failure
Cost: ~$7.68 - dashboard went from $48 to $62, but that included at least one failed run.
c3.2xlarge spot price $0.10 + $0.105, m3.xlarge ~$0.045 + $0.14

20:59 (UTC) start
21:11 container pending drops from 780 to 510 (according to CloudWatch)
21:10 95% CPU utilization according to Ganglia
21:25 first container finishes 
21:30 CPU utilization drops to 70%
21:31 second round of containers started, pending down to 350
21:43 first container of second round finishes
21:45 CPU utilization drops again to 50%
21:50 pending containers down to 200
21:54 222 tasks completed, 107 running, 0 failed, 142 pending (from EMR dashboard)
map tasks taking 18-20 minutes mostly, with some outliers at 30 minutes
22:01 253 tasks complete, 107 running, 0 failed, 111 pending
22:03 container pending <100
22:12 50 containers pending
22:10 285 completed, 107 running, 79 pending, 33 reducers running, 37 pending
22:10 ~40 containers pending, graph flat 'til 22:20
22:13 302 complete, 107 running, 62 pending, still 33 reducers running
22:20 343 complete, 96 running, 32 pending, 33 reducers
22:27 357 complete, 89 running, 25 pending, 46 reducers running
22:32 293 complete, 75 running, 4 pending (all maps), all 70 reducers running
22:35 zero pending tasks
22:40 all tasks complete (not sure how long ago)


2016-04-13 22:37:49,023 INFO org.apache.hadoop.mapreduce.Job (main): Job job_1460581468620_0001 completed successfully
2016-04-13 22:37:49,125 INFO org.apache.hadoop.mapreduce.Job (main): Counters: 80
        File System Counters
                FILE: Number of bytes read=   3592936416
                FILE: Number of bytes written=7749905591
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=91160
                HDFS: Number of bytes written=0
                HDFS: Number of read operations=400
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=0
                S3: Number of bytes read=  360221622683 (360GB)
                S3: Number of bytes written=31037679194  (31GB)
                S3: Number of read operations=0
                S3: Number of large read operations=0
                S3: Number of write operations=0
        Job Counters 
                Launched map tasks=400
                Launched reduce tasks=71
                Data-local map tasks=400
                Total time spent by all maps in occupied slots (ms)=19656362385
                Total time spent by all reduces in occupied slots (ms)=13649966100
                Total time spent by all map tasks (ms)=    436808053 (121hrs, 18.2 min/map avg)
                Total time spent by all reduce tasks (ms)= 151666290 (42 hrs, 36.1 min/reduce avg)
                Total vcore-seconds taken by all map tasks=436808053
                Total vcore-seconds taken by all reduce tasks=151666290
                Total megabyte-seconds taken by all map tasks=629003596320
                Total megabyte-seconds taken by all reduce tasks=436798915200
        Map-Reduce Framework
                Map input records= 59828572
                Map output records=63929664
                Map output bytes=5372975580
                Map output materialized bytes=4097907396
                Input split bytes=91160
                Combine input records=0
                Combine output records=0
                Reduce input groups=251953
                Reduce shuffle bytes=4097907396
                Reduce input records=63929664
                Reduce output records=29890685
                Spilled Records=127859328
                Shuffled Maps =28400
                Failed Shuffles=0
                Merged Map outputs=28400
                GC time elapsed (ms)=3533158
                CPU time spent (ms)=422480670
                Physical memory (bytes) snapshot=441715634176
                Virtual memory (bytes) snapshot=1606043619328
                Total committed heap usage (bytes)=379276754944

        de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$C4_COUNTER
                Map input records=                   59828572 (59.8M)
                M2_WARC_NOT_HTTP_RESPONSE=           39885848
                M3_WARC_WRONG_CONTENT_TYPE=            504128
                M4_WARC_EMPTY_TEXT=                   3456179
                M5_WARC_OUTPUT_RECORDS=              15982416 (15.9M)
                Reduce input records=                63929664 (63.9M)
                R_SIMHASH_CANDIDATE_NOT_DUPLICATE=27146842860
                R_SIMHASH_COMPARISONS=            27165069812 (27Billion)
                R_SIMHASH_EXACT_DUPLICATE=           13255019
                R_SIMHASH_HASH_DIFFERENT_LANGUAGE=     480767
                R_SIMHASH_HASH_DIFFERENT_LENGTH=     15745579
                R_SIMHASH_NEAR_DUPLICATE=            16635666
                R_SIMHASH_NEAR_DUPLICATE_DIFF_LANG=   1591286
        de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$C4_HAMMING_DIST
                D04= 103009309
                D06= 393052862
                D08=1094666125
                D10=2318226500
                D12=3833764476
                D14=5002933159
                D16=5191323091
                D18=4274616529
                D20=2782726049
                D22=1412166159
                D24= 548093229
                D26= 156899445
                D28=  31270013
                D30=   3871677
                D32=    223463
        File Input Format Counters 
                Bytes Read=360221622683
        File Output Format Counters 
                Bytes Written=1242889294
 

Gradual reduction in CPU utilization is probably caused by reduce jobs getting started "too early" is due to this default value:

    mapreduce.job.reduce.slowstart.completedmaps	0.05

Since our Phase 1 is almost a map-only job, we want this to be much closer to 100% (ie 1.0)
*confirmed* - changing this to 0.9 on job submission makes much better use of the CPUs.

=== Run 6 Phase 1 & 2 together ======
2016-04-14 1 m3.xlarge master, 2 m4.4xlarge core, 8 m4.4xlarge task

16:12 UTC Start
318 containers running in first wave
* bad sizing! Too many task nodes for number of tasks should either go up to enough for all mappers in 1 flight
  or down to 2 or more full flights
16:57 first batch of containers finish (~45 min?)
17:03 CPU utilization drops to 50% as first batch finishes
17:14 318 completed, 82 running, 158 pending, no reducers started
17:15 CPU Utilization drops to 20% as second batch finishes
17:19 340 completed, 60 running, 158 pending
17:21 368 complete, 32 running, 158 pending
17:24 375 complete, 25 running, 158 pending - CPU utilization down to 20%
some speculative task executions have been started
17:27 one machine at 70% utilization, one at <50%, all others idling
Need to figure out a way to see if we're being bottlenecked by I/O (S3 bandwidth)
Also whether 100% CPU is max or it's ncore*100%
17:29 382 complete, 165 running, 11 pending - last 15-16 mappers still finishing up, most reducers launched
** too many reducers - (158?) - also means that we get 158 x # licenses x # langs output files (22,510!)
each reducer outputs 127-154 difference language combos (avg 142), all in a single sequence
  (ie Lic_None-Lang_English is spread out enough)
? cut down to 50 reducers to start for 10% case to see if it can be reduced further ?
17:31:19 Map 100%, reduce 32%
17:32 spike in CPU when reducers launched, but now just idle waiting
17:32:54 Reduce 100% complete
17:34 526 completed, 32 running (doesn't match up with detailed task listing which still shows pendings & more runnings)
17:34 Step/Phase 2 job starts
17:35 CPU 90%+, inbound network 380 MB/s huge spike
17:36:28 Map 100% complete, reduce 14%
17:37:45 Reduce 100% complete
17:38:46 Step 2 job complete
17:40 Cluster complete 1hr30 min

Step 1 stats

2016-04-15 17:34:00,713 INFO org.apache.hadoop.mapreduce.Job (main): Job job_1460737149212_0001 completed successfully
2016-04-15 17:34:00,820 INFO org.apache.hadoop.mapreduce.Job (main): Counters: 82
        File System Counters
                FILE: Number of bytes read=   3661390557
                FILE: Number of bytes written=7732362777
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=91160
                HDFS: Number of bytes written=0
                HDFS: Number of read operations=400
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=0
                S3: Number of bytes read=  360221622683 (360 GB)
                S3: Number of bytes written=31062333707 ( 31 GB)
                S3: Number of read operations=0
                S3: Number of large read operations=0
                S3: Number of write operations=0
        Job Counters 
                Killed map tasks=18
                Launched map tasks=418
                Launched reduce tasks=158
                Data-local map tasks=418
                Total time spent by all maps in occupied slots (ms)=  49681884392
                Total time spent by all reduces in occupied slots (ms)=8652317408
                Total time spent by all map tasks (ms)=                 887176507 (246.4 hrs, avg 37 min/map)
                Total time spent by all reduce tasks (ms)=               77252834
                Total vcore-seconds taken by all map tasks=887176507
                Total vcore-seconds taken by all reduce tasks=77252834
                Total megabyte-seconds taken by all map tasks=1589820300544
                Total megabyte-seconds taken by all reduce tasks=276874157056
        Map-Reduce Framework
                Map input records=59828572
                Map output records=63929664
                Map output bytes=6334364556
                Map output materialized bytes=4000340036
                Input split bytes=91160
                Combine input records=0
                Combine output records=0
                Reduce input groups=251891
                Reduce shuffle bytes=4000340036
                Reduce input records=63929664
                Reduce output records=29933477
                Spilled Records=127859328
                Shuffled Maps =63200
                Failed Shuffles=0
                Merged Map outputs=63200
                GC time elapsed (ms)=7246769
                CPU time spent (ms)=468906910
                Physical memory (bytes) snapshot=555300085760
                Virtual memory (bytes) snapshot=2238671302656
                Total committed heap usage (bytes)=695842373632
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$C4_COUNTER
                Map input records=        59828572 (60M)
                M2_WARC_NOT_HTTP_RESPONSE=39885848
                M3_NO_HTTP_HEADER=               1
                M4_WARC_WRONG_CONTENT_TYPE= 504128
                M5_WARC_EMPTY_TEXT=        3456179
                M6_WARC_OUTPUT_RECORDS=   15982416
                R_SIMHASH_CANDIDATE_NOT_DUPLICATE=27160978171
                R_SIMHASH_COMPARISONS=            27179223089 (27 billion)
                R_SIMHASH_EXACT_DUPLICATE=           13283141 (13M)
                R_SIMHASH_HASH_DIFFERENT_LANGUAGE=     483338
                R_SIMHASH_HASH_DIFFERENT_LENGTH=     15714827
                R_SIMHASH_NEAR_DUPLICATE=            16650336 (17M)
                R_SIMHASH_NEAR_DUPLICATE_DIFF_LANG=   1594582
        de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$C4_HAMMING_DIST
                D04= 103099099
                D06= 393440041
                D08=1095524577
                D10=2320344755
                D12=3836024850
                D14=5006744659
                D16=5193026054
                D18=4277051224
                D20=2782855603
                D22=1412630934
                D24= 547971489
                D26= 156919407
                D28=  31250998
                D30=   3870355
                D32=    223347
        File Input Format Counters 
                Bytes Read=360221622683
        File Output Format Counters 
                Bytes Written=1263678323

Step 2 stats

2016-04-15 17:38:46,765 INFO org.apache.hadoop.mapreduce.Job (main): Job job_1460737149212_0002 completed successfully
2016-04-15 17:38:46,898 INFO org.apache.hadoop.mapreduce.Job (main): Counters: 60
        File System Counters
                FILE: Number of bytes read=   120757771813 (121 GB)
                FILE: Number of bytes written=174544760640
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=218614
                HDFS: Number of bytes written=0
                HDFS: Number of read operations=558
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=0
                S3: Number of bytes read=   31062333707 (31 GB)
                S3: Number of bytes written=17740008791 (18 GB)
                S3: Number of read operations=0
                S3: Number of large read operations=0
                S3: Number of write operations=0
        Job Counters 
                Killed map tasks=1
                Launched map tasks=559
                Launched reduce tasks=158
                Data-local map tasks=559
                Total time spent by all maps in occupied slots (ms)=1651025992
                Total time spent by all reduces in occupied slots (ms)=2027226208
                Total time spent by all map tasks (ms)=   29482607
                Total time spent by all reduce tasks (ms)=18100234
                Total vcore-seconds taken by all map tasks=29482607
                Total vcore-seconds taken by all reduce tasks=18100234
                Total megabyte-seconds taken by all map tasks=52832831744
                Total megabyte-seconds taken by all reduce tasks=64871238656
        Map-Reduce Framework
                Map input records=  45915893
                Map output records= 45915893
                Map output bytes=119203740356
                Map output materialized bytes=58224327161
                Input split bytes=218614
                Combine input records=0
                Combine output records=0
                Reduce input groups=15982416
                Reduce shuffle bytes=58224327161
                Reduce input records= 45915893
                Reduce output records= 0 (all records go to side channel)
                Spilled Records=110079629
                Shuffled Maps =88164
                Failed Shuffles=0
                Merged Map outputs=88164
                GC time elapsed (ms)=1231103
                CPU time spent (ms)=19521240
                Physical memory (bytes) snapshot=885913157632
                Virtual memory (bytes) snapshot=2810798084096
                Total committed heap usage (bytes)=1209482084352
        de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase2SortAndDedupe$C4P2_COUNTER
                MAP_EXACT=                 13283141
                MAP_NEAR=                  16650336
                REDUCE_DUPLICATE_COPIES=   38580513
                REDUCE_FILTERED_DUPLICATES= 8647036
                REDUCE_OUTPUT_RECORDS=      7335380

======== Run 7 - scale up to 10% sample with both Steps 1 & 2 together ==

1 m3.xlarge master (@$0.34)+ 6 c3.8xlarge (2 core @$0.70 bid, 64 task @$0.60bid)
spot prices ~$0.40 x 6 = $2.40 + 6 x $0.27 EMR (actual average spot price < $0.40 closer to $0.35?)
estimated cost per hour ~$4.50
total bill before job $97.22, after $140.99 = total cost $44 for 8hr8min (ie 9 hrs) 3136 normalized instance hours
elapsed time 8h8h (of COURSE, just fractionally over the hour boundary)
2016-04-15 
21:14 Start
29:22 End (8hr8min elapsed)
21:23 ~6800 containers pending according to CloudFront (why so many?)
21:37 3695 total tasks, 190 running, 3505 pending, **3600 input files / map tasks       **
21:40 85% CPU, 155 MB/s network (170 MB/s max @startup), 7 hosts, 196 CPUs, 180 GB of 360 GB used
21:44 4 tasks complete, 188 running, 3503 pending (maps 82, 87, 92, 102 completed first)
21:46 114 complete, 187 running, 3394 pending
21:52 181 completed, 190 running, 3324 pending (completed is suspicious because detail shows some running where their logs show complete)
  map times 21-23 minutes from spot sample of first round
  3600 input files / 190 slots = ~19 rounds * 23 minutes = 436 minutes or 7.25 hours
22:08 316 complete, 187 running, 3192 pending
22:10 379 complete (>10% in <47 minutes = ~450 min. estimate), 190 running, 3126 pending
22:21 380 complete
22:29 456 complete, 189 running, 3050 pending
round completions: 21:48, 22:08 22:29
23:12 890 complete, (25% (900) in 110 minutes = ~440 min. estimate) 188 running, 2617 pending
23:49 1170 complete (~1/3 (1200) in 146 minutes = ~438 min. estimate), 190 running 2335 pending
24:49 2209? (had ?3209?) complete, 190 running 1196 pending
00:17 2502 completed, 190 running, 1003 pending (still 0 failed!)
00:20 2686 completed, 190 running, 819 pending (5hrs 8 min elapsed)
00:37 2697 complete (~3/4 in 4 1/4 hrs = est. 340 min, 6:50 = ~27:00 aka 03:00)
00:42 2877 completed, 190 running, 628 pending
00:47 2880 completed, 190 running, 625 pending
01:03 3052 complete, 189 running, 454 pending
01:38 3267 complete (90% in 6.3 hrs == ~ 7hrs), 189 running, 239 pending, 0/95 reducers running
01:41 3304 complete, 187 running, 204 pending
11:55(local) 3542 complete, 123 running, 30 pending, 18 reducers running
11:59(local) 6hrs44minutes elapsed
03:04 map  87%, reduce 0%
03:55 map 100%, reduce 23% (6h45 minutes to complete all maps)
03:58 map 100%, reduce 78%
04:03 map 100% reduce 91%
03:38 (ganglia time) CPU utilization ratchets down from ~85% to ~70%, 5 of 6 nodes less than max utilazation
04:07 3634 completed, 61 running (all reducers), 0 pending, 0 failed, 6h52m all maps complete, 
04:10 3656 complete, 39 running
04:12 3657 complete, 20 running, 0 failed
04:14 3682 complete, 13 running
04:35 3695 complete, 0 running, 0 failed, 0 pending, 0 cancelled (40 min elapsed for reduce phase)
Step 1 complete in 6h54m - 19 rounds, as predicted, rounds stayed distinct in lock step and didn't smudge together in utilization graphs

04:16 Step 2 start - 3695 splits
step 2 inbound network bandwidth peaks at 450 MB/se at startup
step 2 sustained 350-375 MB/s inbound, 250 MB/s outbound
04:24 map 50% reduce 0%
04:31 map 75% reduce 8%
04:33 map 80% reduce 10%
04:39 2715 completed of 3745 total, 145 running, 0 failed, 888 pending
04:38 map 95% reduce 16%
04:40 map 100% reduce 18%
04:43 map 100% reduce 25%
04:46 map 100% reduce 33%
04:47 3694 complete, 51 running, 0 failed, 0 pending, 0 cancelled 
04:50 map 100% reduce 50%
04:53 map 100% reduce 63%
04:49 50 running (all 50 reducers running) -- rewriting entire database in sorted order!
  bottlenecked on reducer capacity outputting to sorted files 
  (conscious tradeoff to minimize number of files)
04:58 map 100% reduce 75%
05:10 3715 complete, 30 running (20 of 50 reducers done)
05:02 map 100% reduce 90% (controller sysylog)
05:08 map 100% reduce 95%
05:13 map 100% reduce 98%
05:15 15 tasks running
05:20 cluster terminating

Step 1 6hr54min (avg map 21.5 min, reduce 40 min. total)
Step 2 1hr3min (14 min. all maps, 50 min. reduce, overlapped with map)

Total time 8hr8min
Total cost $44, ~$5/hr
Estimate for entire crawl 81hrs (on same small 6-node cluster), $405 (at the same spot prices)

Step 1 stats

2016-04-16 04:16:05,852 INFO org.apache.hadoop.mapreduce.Job (main): Job job_1460755206721_0001 completed successfully
2016-04-16 04:16:05,955 INFO org.apache.hadoop.mapreduce.Job (main): Counters: 82
        File System Counters
                FILE: Number of bytes read=   32738767448 (33 GB)
                FILE: Number of bytes written=70746802057 (71 GB)
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=820440
                HDFS: Number of bytes written=0
                HDFS: Number of read operations=3600
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=0
                S3: Number of bytes read=  3240854695944 (3.2 TB)
                S3: Number of bytes written=279212541369 (279 GB)
                S3: Number of read operations=0
                S3: Number of large read operations=0
                S3: Number of write operations=0
        Job Counters 
                Killed map tasks=1
                Launched map tasks=3601
                Launched reduce tasks=95
                Data-local map tasks=3601
                Total time spent by all maps in occupied slots (ms)= 229921991624
                Total time spent by all reduces in occupied slots (ms)=9368770944
                Total time spent by all map tasks (ms)=                4421576762 (20.5 min/map)
                Total time spent by all reduce tasks (ms)=               90084336
                Total vcore-seconds taken by all map tasks=            4421576762
                Total vcore-seconds taken by all reduce tasks=           90084336
                Total megabyte-seconds taken by all map tasks=      7357503731968
                Total megabyte-seconds taken by all reduce tasks=    299800670208
        Map-Reduce Framework
                Map input records= 538184502
                Map output records=575090060
                Map output bytes=             56981843280
                Map output materialized bytes=37541804822
                Input split bytes=820440
                Combine input records=0
                Combine output records=0
                Reduce input groups=261981
                Reduce shuffle bytes=37541804822
                Reduce input records= 575090060
                Reduce output records=285705432 (286M)
                Spilled Records=1150180120
                Shuffled Maps =342000
                Failed Shuffles=0
                Merged Map outputs=342000
                GC time elapsed (ms)=80631123
                CPU time spent (ms)=4422854160
                Physical memory (bytes) snapshot=4082828414976
                Virtual memory (bytes) snapshot=13084950552576
                Total committed heap usage (bytes)=3954309070848
        de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$C4_COUNTER
                Map input records=                   538184502 (538M)
                M2_WARC_NOT_HTTP_RESPONSE=           358790868
                M3_NO_HTTP_HEADER=                           3
                M4_WARC_WRONG_CONTENT_TYPE=            4526545
                M5_WARC_EMPTY_TEXT=                   31094571
                M6_WARC_OUTPUT_RECORDS=              143772515 (144M)
                R_SIMHASH_CANDIDATE_NOT_DUPLICATE=359502913788
                R_SIMHASH_COMPARISONS=            359685188462
                R_SIMHASH_EXACT_DUPLICATE=           132807309
                R_SIMHASH_HASH_DIFFERENT_LANGUAGE=    10676729
                R_SIMHASH_HASH_DIFFERENT_LENGTH=     241220266
                R_SIMHASH_NEAR_DUPLICATE=            152898123
                R_SIMHASH_NEAR_DUPLICATE_DIFF_LANG=   29376551
        de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$C4_HAMMING_DIST
                D04= 1139097042
                D06= 4618740678
                D08=13352217183
                D10=29056957247
                D12=49008352367
                D14=65236807291
                D16=68992843280
                D18=58120175591
                D20=38737660036
                D22=20216975475
                D24= 8075156767
                D26= 2388699390
                D28=  492173613
                D30=   63218346
                D32=    3816026
        File Input Format Counters 
                Bytes Read=3240854695944
        File Output Format Counters 
                Bytes Written=11094940405


Step 2 stats

016-04-16 05:16:17,887 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 100%
2016-04-16 05:19:41,506 INFO org.apache.hadoop.mapreduce.Job (main): Job job_1460755206721_0002 completed successfully
2016-04-16 05:19:41,753 INFO org.apache.hadoop.mapreduce.Job (main): Counters: 60
        File System Counters
                FILE: Number of bytes read=   1072848852800
                FILE: Number of bytes written=1582483663770
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1525635
                HDFS: Number of bytes written=0
                HDFS: Number of read operations=3695
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=0
                S3: Number of bytes read=   279212541369 (279 GB)
                S3: Number of bytes written=179773222782 (180 GB)
                S3: Number of read operations=0
                S3: Number of large read operations=0
                S3: Number of write operations=0
        Job Counters 
                Killed map tasks=2
                Launched map tasks=3697
                Launched reduce tasks=50
                Data-local map tasks=3697
                Total time spent by all maps in occupied slots (ms)=9163909404
                Total time spent by all reduces in occupied slots (ms)=13271137152
                Total time spent by all map tasks (ms)=   176229027
                Total time spent by all reduce tasks (ms)=127607088
                Total vcore-seconds taken by all map tasks=176229027
                Total vcore-seconds taken by all reduce tasks=127607088
                Total megabyte-seconds taken by all map tasks=293245100928
                Total megabyte-seconds taken by all reduce tasks=424676388864
        Map-Reduce Framework
                Map input records= 429477947 (429M = 144M WARCs + 286M duplicates to be deleted)
                Map output records=429477947
                Map output bytes=1078091213591
                Map output materialized bytes=523282217623
                Input split bytes=1525635
                Combine input records=0
                Combine output records=0
                Reduce input groups=    143772515
                Reduce shuffle bytes=523282217623
                Reduce input records=   429477947 (avg 3 per group)
                Reduce output records=0
                Spilled Records=1288433841
                Shuffled Maps =184750
                Failed Shuffles=0
                Merged Map outputs=184750
                GC time elapsed (ms)=8539738
                CPU time spent (ms)=157174510
                Physical memory (bytes) snapshot=3826380689408
                Virtual memory (bytes) snapshot=13180656828416
                Total committed heap usage (bytes)=4967435862016
        de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase2SortAndDedupe$C4P2_COUNTER
                MAP_EXACT=                132807309
                MAP_NEAR=                 152898123
                REDUCE_DUPLICATE_COPIES=  362764917
                Reduce input groups=      143772515 (144M)
                REDUCE_FILTERED_DUPLICATES=77059485 filtered more than half as duplicates
                REDUCE_OUTPUT_RECORDS=     66713030 (66.7 million pages)
        File Input Format Counters 
                Bytes Read=0
        File Output Format Counters 
                Bytes Written=1000

======= Run 8 retest 1% sample with new-dataflow *and missing simhash2 mod* ================== 
1 master + 2 x m4x4xlarge core + 12 x m4.4xlarge task
30 containers per node (30 cores, 52.5 GB used, 3.5 GB available)

Total elapsed time - 57 minutes
Total cost $11.46 (charged for 2 hrs? estimated $5.98/hr)
Step 1 - 40 minutes
  map - 228 hrs, avg 34.2 minutes/task, 10% faster than run 6
Step 2 - 5 minutes
9 min setup time
36 min to complete 100% of maps
1:30 to finish reduces
1:50 to clean up / wind down...

Step 1 stats

2016-04-25 23:11 Step 1 start
2016-04-26 03:51:48,815 INFO org.apache.hadoop.mapreduce.Job (main): Job job_1461640034557_0001 completed successfully
2016-04-26 03:51:48,934 INFO org.apache.hadoop.mapreduce.Job (main): Counters: 97
        File System Counters
                FILE: Number of bytes read=3777086202
                FILE: Number of bytes written=8118173660
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=91160
                HDFS: Number of bytes written=0
                HDFS: Number of read operations=400
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=0
                S3: Number of bytes read=360221622683
                S3: Number of bytes written=30555790829
                S3: Number of read operations=0
                S3: Number of large read operations=0
                S3: Number of write operations=0
        Job Counters 
                Killed map tasks=1
                Launched map tasks=401
                Launched reduce tasks=221
                Data-local map tasks=401
                Total time spent by all maps in occupied slots (ms)=46026018752
                Total time spent by all reduces in occupied slots (ms)=1294476960
                Total time spent by all map tasks (ms)=       821893192 (228 hrs, 34.2 minutes/task average)
                Total time spent by all reduce tasks (ms)=     11557830
                Total vcore-seconds taken by all map tasks=   821893192
                Total vcore-seconds taken by all reduce tasks= 11557830
                Total megabyte-seconds taken by all map tasks=1472832600064
                Total megabyte-seconds taken by all reduce tasks=41423262720
        Map-Reduce Framework
                Map input records=    59828572
                Map output records=   63929664
                Map output bytes=6345002000
                Map output materialized bytes=4262037860
                Input split bytes=91160
                Combine input records=0
                Combine output records=0
                Reduce input groups=262140
                Reduce shuffle bytes=4262037860
                Reduce input records= 63929664
                Reduce output records=19861581
                Spilled Records=127859328
                Shuffled Maps =88400
                Failed Shuffles=0
                Merged Map outputs=88400
                GC time elapsed (ms)=6689488
                CPU time spent (ms)=452396770
                Physical memory (bytes) snapshot=589668790272
                Virtual memory (bytes) snapshot=2562977787904
                Total committed heap usage (bytes)=790892642304
        de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$C4_COUNTER
                Map input records=                   59828572 (60M)
                M2_WARC_NOT_HTTP_RESPONSE=           39885848
                M3_NO_HTTP_HEADER=                          1
                M4_WARC_WRONG_CONTENT_TYPE=            504128
                M5_WARC_EMPTY_TEXT=                   3456179
                M6_WARC_OUTPUT_RECORDS=              15982416 (16M)
                R_SIMHASH_CANDIDATE_NOT_DUPLICATE=35674380150
                R_SIMHASH_COMPARISONS=            35680070932
                R_SIMHASH_EXACT_DUPLICATE=           14392332
                R_SIMHASH_HASH_DIFFERENT_LANGUAGE=     178842
                R_SIMHASH_HASH_DIFFERENT_LENGTH=      5568782
                R_SIMHASH_NEAR_DUPLICATE=             5469249
                R_SIMHASH_NEAR_DUPLICATE_DIFF_LANG=    221533
        de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$C4_HAMMING_DIST
                D03=  11719331
                D04=  29860282
                D05=  69601749
                D06= 148689395
                D07= 289587465
                D08= 511065886
                D09= 816553570
                D10=1187132860
                D11=1584373914
                D12=1962232940
                D13=2281408335
                D14=2517510949
                D15=2663241483
                D16=2722563977
                D17=2704734500
                D18=2619646496
                D19=2474284557
                D20=2274768637
                D21=2028801445
                D22=1746988092
                D23=1444919279
                D24=1141933660
                D25= 858068150
                D26= 610073046
                D27= 408705049
                D28= 256907018
                D29= 150978391
                D30=  82632096
                D31=  41964781
                D32=  19715135
        File Input Format Counters 
                Bytes Read=360221622683
        File Output Format Counters 
                Bytes Written=751967571
  
Step 2 stats

2016-04-26 03:56:50,731 INFO org.apache.hadoop.mapreduce.Job (main): Job job_1461640034557_0002 completed successfully
2016-04-26 03:56:50,859 INFO org.apache.hadoop.mapreduce.Job (main): Counters: 60
        File System Counters
                FILE: Number of bytes read=117392216292
                FILE: Number of bytes written=174165031611
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=240835
                HDFS: Number of bytes written=0
                HDFS: Number of read operations=621
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=0
                S3: Number of bytes read=   30555790829 (30.6 GB)
                S3: Number of bytes written=24714830696 (24.7 GB)
                S3: Number of read operations=0
                S3: Number of large read operations=0
                S3: Number of write operations=0
        Job Counters 
                Killed map tasks=1
                Launched map tasks=622
                Launched reduce tasks=50
                Data-local map tasks=622
                Total time spent by all maps in occupied slots (ms)=1594870200
                Total time spent by all reduces in occupied slots (ms)=959380800
                Total time spent by all map tasks (ms)=   28479825
                Total time spent by all reduce tasks (ms)= 8565900
                Total vcore-seconds taken by all map tasks=28479825
                Total vcore-seconds taken by all reduce tasks=8565900
                Total megabyte-seconds taken by all map tasks=51035846400
                Total megabyte-seconds taken by all reduce tasks=30700185600
        Map-Reduce Framework
                Map input records=35843997
                Map output records=35843997
                Map output bytes=115912631800
                Map output materialized bytes=58125851861
                Input split bytes=240835
                Combine input records=0
                Combine output records=0
                Reduce input groups=15982416
                Reduce shuffle bytes=58125851861
                Reduce input records=35843997
                Reduce output records=0
                Spilled Records=87670410
                Shuffled Maps =31050
                Failed Shuffles=0
                Merged Map outputs=31050
                GC time elapsed (ms)=1114724
                CPU time spent (ms)=17348520
                Physical memory (bytes) snapshot=796978995200
                Virtual memory (bytes) snapshot=2473356546048
                Total committed heap usage (bytes)=1000964358144
        de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase2SortAndDedupe$C4P2_COUNTER
                MAP_EXACT=                 14392332
                MAP_NEAR=                   5469249
                REDUCE_DUPLICATE_COPIES=   25980482
                REDUCE_FILTERED_DUPLICATES= 6118901
                REDUCE_OUTPUT_RECORDS=      9863515
        File Input Format Counters 
                Bytes Read=0
        File Output Format Counters 
                Bytes Written=1000

==== Run 9 10% sample with correct simhash2 
1 m3.xlarge master, 2 c3.8xlarge core @$0.61, 14 c3.8xlarge task @$0.51
$158.35 start $10.26/hr est.
Elapsed 4hr 45 min
$182.90 posted next morning, but didn't include all charges
$211.35 end $53 total

511 containers, 32 containers per node
3853 total tasks = 3600 map tasks + 253 reducers
7 waves of mappers @20 min each = est. 140 min 2hr20min
actual map complete in 2h35m = 25hrs est. total for 16 c3.8xlarge

22:08 (UTC-4) cluster created
02:18 Ganglia shows cluster alive
22:24 cluster provisioned (although Step 1 showed immediately as having been running for 5 min.)
02:23 cluster up to full load
02:26 network traffic at 450 MB/s inbound aggregate (peaks @465 MB/s, > 440MB/s for 4 min, avg 380 MB/s)
02:43 1st map wave complete
03:02 2nd map wave complete
03:09 938 tasks complete, 508 running, 2407 pending, 0 failed or cancelled
      Ganglia solid 94% CPU utilization, mappers ~20 min avg. based on memory utilization
      outbound traffic burst < 4 min w/peak @327 MB/s)
03:20 988 complete, 510 running, 2355 pending
03:21 3rd map wave complete
03:34 1497 complete, 510 running, 1846 pending - mappers almost 50% done after 1h27m
03:41 4th map wave complete
11:42 1540 complete, 510 running, 1803 pending - 90 min elapsed, 80 min remaining (4 waves @20 min.)
11:50 1930 complete, 507 running, 1416 pending - 102 min elapsed
est 03:58 next wave completes
est 04:40 mappers complete
04:00 5th wave complete
04:19 6th wave complete
23:20 2691 complete, 508 running, 654 pending (current wave + ~400 maps to complete)
est 4:50-4:59 all mappers complete
23:27 2972 complete, 510 running, 371 pending
  mapper cadence looks closer to 19 min than 20 min
  throughput could probably be improved by staggered start
23:36 Ganglia CPU utilization drops dramaticall from 95% at 04:36 to 25% at 04:39
23:38 3167 complete, 433 running, 253 pending (all reducers)
23:38 Ganglia Network traffic huge spike 667 MB/s in, 620 MB/s out (feeding shufflers/reducers?)
23:44 reducers running
23:45 3470 complete, 313 running, 70 pending (why pending when we have slots available? 95% cutoff?)
04:40 Network in traffic drops from 400 MB/s to 120 MB/s as reducers take over (or mappers tail off?)
04:46 CPU utilization drops from 20-25% to ...
04:46 Network traffic outbound rises gradually to 75 MB/s and inbound drops to about the same, on the way to zero
23:51 3533 complete, 284 running, 36 pending -- some map stragglers still running, some reducers pending
23:54 3580 complete, 265 running, 8 pending
23:56 3600 complete, 253 running, 0 pending
04:38-04:54 ramp down to Step 1 reduce phase before Step 2 begins at 04:54 (15-20 min. mostly wasted)
04:53 map phase done, steep ramp up in CPU utilization to reduce phase startup, topping out at 45-50% CPU, then decaying
23:59 3609 complete, 244 running
05:00 map 100%, reduce 87%
24:05 3644 complete, 209 running
05:05 CPU utilization steady downward ramp after up spike as reduce phase starts 00:55 = 48%, 01:00 = 45%, 01:05 = 33%, 01:10=21%
05:06 mapp 100%, reduce 93%
25:07 3677 complete, 176 running
25:08 3709 complete, 144 running
25:10 CPU utlization 21%
25:15 3765 complete, 88 running
05:15 map 100% reduce 98%
25:21 3798 complete, 55 running
05:20 Ganglia CPU utilization down to 10%
04:54-05:22 network traffic almost completely flat (tiny outbound bump at 05:16, no inbound traffic at all)
24:24 3802 complete, 51 running, 0 pending
05:20 map 100%, reduce 99%
25:28 3827 complete, 26 running
25:30 3-0 containers running per node (21 total for 16 nodes)
25:32 3833 complete, 20 running
05:33 map 100%, reduce 100% (but not done!)
25:39 3843 complete, 10 running (3x2per node + 4 * 1 per node)
25:41 3844 complete, 9 running
25:47 3846 complete, 7 running (of 253 reducers)
** Check logs for long running reducers for this completion string:
**      "Reducer complete - %d msec elapsed, key count=%d, max values = %d, key for max values = %d, total comparisons = %d",
25:56 3847 complete, 6 running
25:59 3849 complete, 4 running (reducers 105, 159, 223, 247)
reducer 105 = 378K values for key 109113644154880, 3973 seconds elapsed (66 min.), 27880534491 (28B comparisons)
reducer 159 = 723K values for key           20905, 4205 seconds elapsed (70 min.), 20059563887 (20B) comparisons
26:09 3851 complete, 2 running (223 & 247 still running - 247 killed at timelimit (1hr?), 223 cancelled when job terminated by me )
50k^2 = 2.5B
Reducer complete - 3972857 msec elapsed, key count=1038, max values = 378086, key for max values = 109113644154880, total comparisons = 27880534491
2016-04-27 06:00:10,251 INFO [main] com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream: close closed:false s3://tfmorris/c4corpus/cc-phase1out-2016-07-10pct-new-dataflow2/part-r-00105.gz
Reducer complete - 4205292 msec elapsed, key count=1038, max values = 723328, key for max values = 20905, total comparisons = 20059563887
2016-04-27 06:04:03,358 INFO [main] com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream: close closed:false s3://tfmorris/c4corpus/cc-phase1out-2016-07-10pct-new-dataflow2/part-r-00159.gz


===== Run 10

20 x c3.8xlarge bid @$0.41, est. $11.34/hr @$0.28, est 4hrs == $45.34
New 11(+)% tranche of dataset
Creation date:2016-05-01 21:15 (UTC-4)

jar: dkpro-c4corpus-hadoop-1.0.1-SNAPSHOT-standalone.jar 
class: de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob 
-D mapreduce.task.timeout=7200000 
-D mapreduce.map.failures.maxpercent=5 
-D mapreduce.map.maxattempts=2 
-D mapreduce.job.reduce.slowstart.completedmaps=0.95 
-D c4corpus.keepminimalhtml=true 
input: s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/*/warc/*-00[0-3]1[0-9]-*.warc.gz 
output: s3://tfmorris/c4corpus/cc-phase1out-2016-07-10pct-new-dataflow2-2

Step 1 - 4063 total tasks- 4000 maps (up from 3600 for 10% sample), 63 reducers (why so few?)
estimate 6.3 rounds, 2 1/3 hrs (doesn't account for slow start for first 10 min.), actual 2h52 min.
 Map 2h52 min, avg map time 19.3 min, reduce phase ~35 min
 
Step 2 - start 04:19, 5 min CPU peak (map phase?), 3.6 GB/sec inbound peak 4:22:30, 3.2 GB/s outbound peak 2 min. later 4:24:30

Step 1 waves ~19 min 02:02, 02:21, 02:39, 03:02
23:11 2648 complete, 638 running, 777 pending, 0 failed, 0 cancelled 1h54 min elapsed (but most task nodes delayed 20 min)
23:50 3892 complete, 171 running
03:36 ramp down from 90%+ to almost zero at 03:50
03:39:50,390 map 97%, reduce 5%
03:40 huge inbound/outbound traffic spike in Ganglia
03:44:20,383 map 99% reduce 33%
03:48:45,105 map 100% reduce 33%
11:54 4000 complete, 63 running
03:52:36,725 map 100% reduce 72%
reduce tasks running 4 per node, 3 nodes empty (master & 2 core?)
03:57:45,598 map 100% reduce 81%
04:02:34,396 map 100% reduce 89%
04:03:10,510 map 100% reduce 90%
04:06:08,009 map 100% reduce 95%
04:07:24,218 map 100% reduce 97%
04:07 reducers start finishing
04:10 most reducers finished
04:08:12,339 map 100% reduce 98%
04:09:18,505 map 100% reduce 99%
04:11:39,863 map 100% reduce 100%
12:12 4048 complete, 15 running, 0 failed, 0 canceled
12:14  complete, 9 running, 2h59m elapsed (Step 2 still pending)
12:17 4062 complete, 1 running r_00005 last task running

2016-05-02 03:50:11,978 INFO [main] de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$SimhashSimilarityReducer: Reducer setup starting
2016-05-02 03:50:14,594 INFO [main] amazon.emr.metrics.MetricsSaver: 1 MetricsLockFreeSaver 1 comitted 23 matured S3WriteBytes values
2016-05-02 03:50:43,006  Exceeded comparison limit 100000291 for key 918d000000000000 after  6568 docs. N= 19724 for N^2 phase
2016-05-02 03:51:00,889  Exceeded comparison limit 100002943 for key 98ef000000000000 after  8889 docs. N= 17175 for N^2 phase
2016-05-02 03:51:20,654  Exceeded comparison limit 100004348 for key c08e000000000000 after  8646 docs. N= 18735 for N^2 phase
2016-05-02 03:51:48,614  Exceeded comparison limit 100003481 for key d00f000000000000 after  8815 docs. N= 17548 for N^2 phase
2016-05-02 03:51:59,413  Exceeded comparison limit 100023114 for key d08d000000000000 after  2809 docs. N= 39648 for N^2 phase
2016-05-02 03:53:49,919  Exceeded comparison limit 100075668 for key             51b9 after   830 docs. N=141166 for N^2 phase ** (349K before exact match dedupe)
2016-05-02 03:54:10,401  Exceeded comparison limit 100002449 for key             55a9 after  3549 docs. N= 35133 for N^2 phase
2016-05-02 03:54:22,426  Exceeded comparison limit 100004514 for key             5999 after  2633 docs. N= 42605 for N^2 phase
2016-05-02 03:54:57,009  Exceeded comparison limit 100021597 for key             91b5 after  4152 docs. N= 30375 for N^2 phase
2016-05-02 03:55:06,995  Exceeded comparison limit 100004222 for key             92b1 after  4691 docs. N= 28374 for N^2 phase
2016-05-02 03:55:18,023  Exceeded comparison limit 100027120 for key             93ad after  3380 docs. N= 34379 for N^2 phase
2016-05-02 03:55:45,672  Exceeded comparison limit 100003652 for key             9a91 after  8715 docs. N= 19081 for N^2 phase
2016-05-02 03:56:37,421  Exceeded comparison limit 100024479 for key             d1b1 after  2613 docs. N= 48608 for N^2 phase
2016-05-02 03:56:51,229  Exceeded comparison limit 100029838 for key             d3a9 after  1667 docs. N= 64831 for N^2 phase
2016-05-02 03:57:14,573  Exceeded comparison limit 100006210 for key             d991 after  7976 docs. N= 19642 for N^2 phase
2016-05-02 03:57:24,209  Exceeded comparison limit 100004048 for key             db89 after  6842 docs. N= 20757 for N^2 phase
2016-05-02 03:58:32,929  Exceeded comparison limit 100003003 for key         36770000 after 10067 docs. N= 17364 for N^2 phase
2016-05-02 03:58:44,084  Exceeded comparison limit 100000725 for key         36f50000 after  5037 docs. N= 24360 for N^2 phase
2016-05-02 03:59:02,567  Exceeded comparison limit 100002514 for key         46760000 after 10764 docs. N= 16844 for N^2 phase
2016-05-02 03:59:22,491  Exceeded comparison limit 100042419 for key         46f40000 after  1846 docs. N= 66355 for N^2 phase
2016-05-02 03:59:35,096  Exceeded comparison limit 100008877 for key         547d0000 after  8703 docs. N= 17807 for N^2 phase
2016-05-02 03:59:58,474  Exceeded comparison limit 100031844 for key         56750000 after   949 docs. N=115197 for N^2 phase **
2016-05-02 04:00:06,992  Exceeded comparison limit 100014330 for key         56b40000 after 5409 docs. N=22430 for N^2 phase
2016-05-02 04:00:17,114  Exceeded comparison limit 100008844 for key         56f30000 after 6683 docs. N=19203 for N^2 phase
2016-05-02 04:00:53,344  Exceeded comparison limit 100053229 for key         66740000 after 1529 docs. N=71574 for N^2 phase
2016-05-02 04:01:22,759  Exceeded comparison limit 100015632 for key         76340000 after 4302 docs. N=27983 for N^2 phase
2016-05-02 04:01:32,290  Exceeded comparison limit 100004390 for key         76730000 after 6327 docs. N=20387 for N^2 phase
2016-05-02 04:01:42,687  Exceeded comparison limit 100013122 for key         76f10000 after 3713 docs. N=30523 for N^2 phase
2016-05-02 04:01:57,631  Exceeded comparison limit 100002251 for key         86f80000 after 12052 docs. N=19103 for N^2 phase
2016-05-02 04:02:40,984  Exceeded comparison limit 100002549 for key         c6f40000 after 2149 docs. N=51601 for N^2 phase
2016-05-02 04:02:56,428  Exceeded comparison limit 100007329 for key         d6750000 after 3294 docs. N=34417 for N^2 phase
2016-05-02 04:03:34,256  Exceeded comparison limit 100009122 for key         e6740000 after 2149 docs. N=51473 for N^2 phase
2016-05-02 04:03:54,413  Exceeded comparison limit 100006876 for key         f6340000 after 5938 docs. N=21146 for N^2 phase
2016-05-02 04:04:34,257  Exceeded comparison limit 100031692 for key     217d00000000 after  2900 docs. N= 43634 for N^2 phase
2016-05-02 04:04:45,366  Exceeded comparison limit 100001532 for key     21fb00000000 after  5755 docs. N= 22942 for N^2 phase
2016-05-02 04:04:54,839  Exceeded comparison limit 100006052 for key     227900000000 after  6557 docs. N= 19393 for N^2 phase
2016-05-02 04:05:08,266  Exceeded comparison limit 100010343 for key     237500000000 after  4109 docs. N= 27892 for N^2 phase
2016-05-02 04:05:33,667  Exceeded comparison limit 100040432 for key     313d00000000 after  1978 docs. N= 57047 for N^2 phase
2016-05-02 04:05:44,017  Exceeded comparison limit 100009018 for key     31bb00000000 after  4509 docs. N= 26085 for N^2 phase
2016-05-02 04:05:56,302  Exceeded comparison limit 100009121 for key     323900000000 after  7854 docs. N= 17477 for N^2 phase
2016-05-02 04:06:11,286  Exceeded comparison limit 100015164 for key     333500000000 after  4065 docs. N= 28758 for N^2 phase
2016-05-02 04:07:02,509  Exceeded comparison limit 100027712 for key     607d00000000 after  2906 docs. N= 36987 for N^2 phase
2016-05-02 04:07:15,718  Exceeded comparison limit 100027992 for key     617900000000 after  2611 docs. N= 43021 for N^2 phase
2016-05-02 04:07:32,935  Exceeded comparison limit 100009065 for key     627500000000 after  3899 docs. N= 30214 for N^2 phase
2016-05-02 04:08:00,668  Exceeded comparison limit 100023167 for key     703d00000000 after  3412 docs. N= 31917 for N^2 phase
2016-05-02 04:08:16,594  Exceeded comparison limit 100018740 for key     713900000000 after  2683 docs. N= 41764 for N^2 phase
2016-05-02 04:08:28,311  Exceeded comparison limit 100000850 for key     723500000000 after  6512 docs. N= 19210 for N^2 phase
2016-05-02 04:09:35,356  Exceeded comparison limit 100009896 for key  18e000000000000 after  3015 docs. N= 38493 for N^2 phase
2016-05-02 04:10:03,749  Exceeded comparison limit 100013572 for key 110f000000000000 after  5592 docs. N= 23361 for N^2 phase
2016-05-02 04:10:14,475  Exceeded comparison limit 100026027 for key 118d000000000000 after  2349 docs. N= 46840 for N^2 phase
2016-05-02 04:10:23,071  Exceeded comparison limit 100001154 for key 11cc000000000000 after  9840 docs. N= 16512 for N^2 phase
2016-05-02 04:10:49,494  Exceeded comparison limit 100009292 for key 18ef000000000000 after  7345 docs. N= 18385 for N^2 phase
2016-05-02 04:11:36,289  Exceeded comparison limit 100076829 for key 408e000000000000 after  1231 docs. N= 90444 for N^2 phase
2016-05-02 04:11:44,780  Exceeded comparison limit 100002186 for key 40cd000000000000 after  4531 docs. N= 26610 for N^2 phase
2016-05-02 04:11:53,809  Exceeded comparison limit 100029287 for key 410c000000000000 after  2238 docs. N=  1905 for N^2 phase ??
2016-05-02 04:12:10,500  Exceeded comparison limit 100000739 for key 48ad000000000000 after 13540 docs. N= 14772 for N^2 phase
2016-05-02 04:12:30,830  Exceeded comparison limit 100053878 for key 500f000000000000 after  1922 docs. N= 58565 for N^2 phase
2016-05-02 04:12:56,218  Exceeded comparison limit 100113751 for key 508d000000000000 after   752 docs. N=143602 for N^2 phase **
2016-05-02 04:13:05,158  Exceeded comparison limit 100025614 for key 50cc000000000000 after  2631 docs. N= 44569 for N^2 phase
2016-05-02 04:13:18,877  Exceeded comparison limit 100006326 for key 5189000000000000 after  8031 docs. N= 18358 for N^2 phase
2016-05-02 04:13:40,200  Exceeded comparison limit 100001722 for key 586d000000000000 after 11762 docs. N= 15351 for N^2 phase
2016-05-02 04:13:50,369  Exceeded comparison limit 100005796 for key 58ac000000000000 after  8258 docs. N= 17922 for N^2 phase
2016-05-02 04:14:13,423  Exceeded comparison limit 100003914 for key 600e000000000000 after  4342 docs. N= 31280 for N^2 phase
2016-05-02 04:14:23,243  Exceeded comparison limit 100001375 for key 608c000000000000 after  5530 docs. N= 24611 for N^2 phase
2016-05-02 04:14:40,859 Reducer complete - 1468880 msec elapsed 24.5 min., key count=4162, max values = 349116, key for max values = 51b9, total comparisons = 14438920051 (14.4B)

2016-05-02 04:11:39,863 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 100%
2016-05-02 04:18:20,898 INFO org.apache.hadoop.mapreduce.Job (main): Job job_1462152250076_0001 completed successfully
2016-05-02 04:18:21,149 INFO org.apache.hadoop.mapreduce.Job (main): Counters: 99
        File System Counters
                FILE: Number of bytes read=37667830119
                FILE: Number of bytes written=80694155705
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=911600
                HDFS: Number of bytes written=0
                HDFS: Number of read operations=4000
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=0
                S3: Number of bytes read=3600075628740
                S3: Number of bytes written=305732844053
                S3: Number of read operations=0
                S3: Number of large read operations=0
                S3: Number of write operations=0
        Job Counters 
                Failed map tasks=       2
                Killed map tasks=      12
                Launched map tasks=  4014
                Launched reduce tasks= 63
                Other local map tasks=  2
                Data-local map tasks=4012
                Total time spent by all maps in occupied slots (ms)=240918430324
                Total time spent by all reduces in occupied slots (ms)=11741757768
                Total time spent by all map tasks (ms)=   4633046737 (1287 hrs, avg 19.3 min/map task)
                Total time spent by all reduce tasks (ms)= 112901517 (31.4 hrs total, ~35 min elapsed for reduce phase)
                Total vcore-seconds taken by all map tasks=4633046737
                Total vcore-seconds taken by all reduce tasks=112901517
                Total megabyte-seconds taken by all map tasks=7709389770368
                Total megabyte-seconds taken by all reduce tasks=375736248576
        Map-Reduce Framework
                Map input records= 597958999
                Map output records=638962160
                Map output bytes=63416839820
                Map output materialized bytes=42516511453
                Input split bytes=911600
                Combine input records=0
                Combine output records=0
                Reduce input groups=262141
                Reduce shuffle bytes=42516511453
                Reduce input records= 638962160
                Reduce output records=220432855
                Spilled Records=1277924320
                Shuffled Maps =252000
                Failed Shuffles=0
                Merged Map outputs=252000
                GC time elapsed (ms)=77154439
                CPU time spent (ms)=4607261380
                Physical memory (bytes) snapshot=4331295092736
                Virtual memory (bytes) snapshot=14332757790720
                Total committed heap usage (bytes)=4262669058048
        de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$C4_COUNTER
                Map input records=        597958999 (598M)
                M2_WARC_NOT_HTTP_RESPONSE=398640666
                M3_NO_HTTP_HEADER=                4
                M4_WARC_WRONG_CONTENT_TYPE= 5025965
                M5_WARC_EMPTY_TEXT=        34551824
                M6_WARC_OUTPUT_RECORDS=   159740540 (160M)
                R_SIMHASH_CANDIDATE_NOT_DUPLICATE= 699426426548
                R_SIMHASH_COMPARISONS=             699470690119
                R_SIMHASH_EXACT_DUPLICATE=            177554380 (176M)
                R_SIMHASH_HASH_DIFFERENT_LANGUAGE=      2300317
                R_SIMHASH_HASH_DIFFERENT_LENGTH=       82428327
                R_SIMHASH_NEAR_DUPLICATE=              42878475
                R_SIMHASH_NEAR_DUPLICATE_DIFF_LANG=     1385096
        de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob$C4_HAMMING_DIST
                D03=92971733
                D04=227203498
                D05=475279413
                D06=879452949
                D07=1496320567
                D08=2422004573
                D09=3807333117
                D10=5848286359
                D11=8751111716
                D12=12687784330
                D13=17755805071
                D14=23937475205
                D15=31067091547
                D16=38797695747
                D17=46577859901
                D18=53664036583
                D19=59195159555
                D20=62330919504
                D21=62454010174
                D22=59352243633
                D23=53329033920
                D24=45167040385
                D25=35956748235
                D26=26831602299
                D27=18719913805
                D28=12179269038
                D29=7370245071
                D30=4137676067
                D31=2148887128
                D32=1029295529
        File Input Format Counters 
                Bytes Read=3600075628740
        File Output Format Counters 
                Bytes Written=7844703896


Step 2 - 26 min elapsed

4062 map tasks, 50 reducers
12:40 4113 total tasks - 3127 completed, 376 running, 0 failed, 610 pending, 0 cancelled. 2h24 min

2016-05-02 04:18:39,892 INFO org.apache.hadoop.mapreduce.Job (main): Job job_1462152250076_0002 running in uber mode : false
2016-05-02 04:18:39,893 INFO org.apache.hadoop.mapreduce.Job (main):  map 0% reduce 0%
2016-05-02 04:18:56,101 INFO org.apache.hadoop.mapreduce.Job (main):  map 1% reduce 0%
2016-05-02 04:19:09,455 INFO org.apache.hadoop.mapreduce.Job (main):  map 5% reduce 0%
2016-05-02 04:19:52,897 INFO org.apache.hadoop.mapreduce.Job (main):  map 20% reduce 0%

2016-05-02 04:20:23,967 INFO org.apache.hadoop.mapreduce.Job (main):  map 30% reduce 0%
2016-05-02 04:20:55,243 INFO org.apache.hadoop.mapreduce.Job (main):  map 40% reduce 0%
2016-05-02 04:21:17,662 INFO org.apache.hadoop.mapreduce.Job (main):  map 50% reduce 0%
2016-05-02 04:21:47,419 INFO org.apache.hadoop.mapreduce.Job (main):  map 60% reduce 0%
2016-05-02 04:22:26,732 INFO org.apache.hadoop.mapreduce.Job (main):  map 69% reduce 4%

2016-05-02 04:22:46,296 INFO org.apache.hadoop.mapreduce.Job (main): Task Id : attempt_1462152250076_0002_m_002616_0, Status : FAILED

2016-05-02 04:22:58,486 INFO org.apache.hadoop.mapreduce.Job (main):  map 80% reduce 10%
2016-05-02 04:23:12,781 INFO org.apache.hadoop.mapreduce.Job (main):  map 85% reduce 11%
2016-05-02 04:23:27,256 INFO org.apache.hadoop.mapreduce.Job (main):  map 90% reduce 13%
2016-05-02 04:23:47,714 INFO org.apache.hadoop.mapreduce.Job (main):  map 95% reduce 14%

2016-05-02 04:23:55,133 INFO org.apache.hadoop.mapreduce.Job (main): Task Id : attempt_1462152250076_0002_m_003196_0, Status : FAILED
2016-05-02 04:24:08,587 INFO org.apache.hadoop.mapreduce.Job (main): Task Id : attempt_1462152250076_0002_m_003246_0, Status : FAILED

2016-05-02 04:24:19,836 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 20%
2016-05-02 04:25:20,891 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 30%
2016-05-02 04:25:51,269 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 40%
2016-05-02 04:26:32,519 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 50%
2016-05-02 04:27:39,830 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 60%
2016-05-02 04:28:17,992 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 70%
2016-05-02 04:32:32,099 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 80%

2016-05-02 04:34:32,610 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 85%
2016-05-02 04:36:36,101 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 90%

2016-05-02 04:38:38,637 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 95%
2016-05-02 04:39:06,767 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 96%
2016-05-02 04:39:37,900 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 97%
2016-05-02 04:40:14,053 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 98%
2016-05-02 04:41:05,270 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 99%
2016-05-02 04:42:18,579 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 100%
2016-05-02 04:42:18,579 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 100%
2016-05-02 04:44:49,209 INFO org.apache.hadoop.mapreduce.Job (main): Job job_1462152250076_0002 completed successfully
2016-05-02 04:44:49,457 INFO org.apache.hadoop.mapreduce.Job (main): Counters: 62
        File System Counters
                FILE: Number of bytes read=1189195440834
                FILE: Number of bytes written=1756760117298
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1693168
                HDFS: Number of bytes written=0
                HDFS: Number of read operations=4063
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=0
                S3: Number of bytes read=305732844053
                S3: Number of bytes written=228072404481
                S3: Number of read operations=0
                S3: Number of large read operations=0
                S3: Number of write operations=0
        Job Counters 
                Failed map tasks=1
                Killed map tasks=3
                Launched map tasks=4067
                Launched reduce tasks=50
                Other local map tasks=3
                Data-local map tasks=4066
                Total time spent by all maps in occupied slots (ms)=8763032512
                Total time spent by all reduces in occupied slots (ms)=6432360168
                Total time spent by all map tasks (ms)=  168519856
                Total time spent by all reduce tasks (ms)=61849617
                Total vcore-seconds taken by all map tasks=168519856
                Total vcore-seconds taken by all reduce tasks=61849617
                Total megabyte-seconds taken by all map tasks=280417040384
                Total megabyte-seconds taken by all reduce tasks=205835525376
        Map-Reduce Framework
                Map input records= 380173395
                Map output records=380173395
                Map output bytes=1165824756002
                Map output materialized bytes=581833983635
                Input split bytes=1693168
                Combine input records=0
                Combine output records=0
                Reduce input groups=    159740540
                Reduce shuffle bytes=581833983635
                Reduce input records=   380173395
                Reduce output records=0
                Spilled Records=1140520185
                Shuffled Maps =203150
                Failed Shuffles=0
                Merged Map outputs=203150
                GC time elapsed (ms)=10370632
                CPU time spent (ms)=169174630
                Physical memory (bytes) snapshot=4163600961536
                Virtual memory (bytes) snapshot=14471292493824
                Total committed heap usage (bytes)=5444804280320
        de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase2SortAndDedupe$C4P2_COUNTER
                MAP_EXACT= 177554380
                MAP_NEAR=   42878475
                REDUCE_DUPLICATE_COPIES=  290579582
                REDUCE_FILTERED_DUPLICATES=70146727 (70M dupes)
                REDUCE_OUTPUT_RECORDS=     89593813 (90M deduped pages)


